{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf356600",
   "metadata": {},
   "source": [
    "Semantic Cluster Labeling a required step that sits before the classification-with-categories. \n",
    "Unsupervised learning - \n",
    "This step will (a) describe each cluster with top keywords and top genres, \n",
    "(b) give each cluster a short human-readable label, and \n",
    "(c) save tidy outputs for your slides and for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f2049",
   "metadata": {},
   "source": [
    "Loads your clustered data (data/shows_with_umap_kmeans.parquet) and the review info (data/shows_for_review.csv)\n",
    "Merges them to get name, genres, ai_summary, u1/u2, cluster\n",
    "Builds TF-IDF over all summaries and computes per-cluster top unigrams/bigrams\n",
    "Extracts top genres per cluster (from TVMaze)\n",
    "Creates a cluster_label (e.g., \"crime, investigation, police\")\n",
    "\n",
    "Saves:\n",
    "data/shows_with_cluster_labels.parquet (same rows, plus cluster_label)\n",
    "data/cluster_profiles.csv (one row per cluster, with keywords, genres, examples)\n",
    "reports/cluster_profiles.md (nice markdown for slides/notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d594570",
   "metadata": {},
   "source": [
    "üìå How to position this in your pipeline\n",
    "Fetch & clean ‚Üí raw_tvmaze.jsonl\n",
    "Summarize ‚Üí shows.parquet\n",
    "Merge (keep original + AI summary) ‚Üí shows_merged.parquet + shows_for_review.csv\n",
    "Embed ‚Üí vectors/summaries.npy + vectors/summaries_index.parquet\n",
    "UMAP + K-Means ‚Üí shows_with_umap_kmeans.parquet + plot\n",
    "üëâ Semantic Cluster Labeling (this step, mandatory) ‚Üí\n",
    "shows_with_cluster_labels.parquet, cluster_profiles.csv, cluster_profiles.md\n",
    "(Later) Supervised classification ‚Üí compare to your cluster labels / categories\n",
    "\n",
    "üéØ Why this is important (and belongs before supervised classification)\n",
    "Gives you human-interpretable descriptions of unsupervised groups\n",
    "Helps you choose/adjust your predefined categories (ground them in the data)\n",
    "Lets you sanity-check and explain your clusters in slides\n",
    "\n",
    "If you want, I can also add a snippet that uses those cluster_labels to seed your category list automatically (e.g., pick the top keywords/genres and propose category names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e3b0f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bc9070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "REPORTS = Path(\"reports\"); REPORTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Load clustered coords/labels (from your UMAP + KMeans step)\n",
    "clustered = pd.read_parquet(DATA_DIR / \"shows_with_umap_kmeans.parquet\")   # id, name, u1, u2, cluster\n",
    "\n",
    "# ---- Load review info (names, genres, both summaries)\n",
    "review = pd.read_csv(DATA_DIR / \"shows_for_review.csv\")  # id, name, genres, original_summary, ai_summary\n",
    "\n",
    "# ---- Merge\n",
    "df = pd.merge(\n",
    "    review,\n",
    "    clustered[[\"id\", \"u1\", \"u2\", \"cluster\"]],\n",
    "    on=\"id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# ---- Ensure 'genres' is a list (CSV often stores it as a string)\n",
    "def to_list_maybe(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip()\n",
    "        # Try to parse JSON-like list\n",
    "        if x.startswith(\"[\") and x.endswith(\"]\"):\n",
    "            try:\n",
    "                return list(ast.literal_eval(x))\n",
    "            except Exception:\n",
    "                pass\n",
    "        # Fallback: split on commas\n",
    "        return [t.strip() for t in x.split(\",\")] if x else []\n",
    "    return []\n",
    "\n",
    "df[\"genres\"] = df[\"genres\"].apply(to_list_maybe)\n",
    "\n",
    "# ---- Basic sanity\n",
    "assert \"ai_summary\" in df.columns, \"ai_summary column missing. Make sure you merged after make_summaries.\"\n",
    "assert \"cluster\" in df.columns, \"cluster column missing. Run UMAP + KMeans first.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdd20941",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Build a TF-IDF model over ALL summaries, then summarize per cluster\n",
    "texts_all = df[\"ai_summary\"].fillna(\"\").astype(str).values\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,      # big enough to get signal, small enough to be fast\n",
    "    ngram_range=(1,2),      # unigrams + bigrams\n",
    "    min_df=2,               # ignore rare terms\n",
    "    max_df=0.8,             # drop very common terms\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "X_tfidf = vectorizer.fit_transform(texts_all)\n",
    "terms = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "def top_keywords_for_cluster(rows_idx, topk=10):\n",
    "    # average TF-IDF within the cluster, then take top-k features\n",
    "    if len(rows_idx) == 0:\n",
    "        return []\n",
    "    mean_tfidf = X_tfidf[rows_idx].mean(axis=0).A1\n",
    "    top_idx = np.argsort(-mean_tfidf)[:topk]\n",
    "    return terms[top_idx].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aac9a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Top genres per cluster\n",
    "def top_genres_for_cluster(sub, k=3):\n",
    "    flat = [g for lst in sub[\"genres\"].dropna() for g in (lst if isinstance(lst, list) else [])]\n",
    "    if not flat:\n",
    "        return []\n",
    "    return pd.Series(flat).value_counts().head(k).index.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b12dba42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Cluster profiles preview:\n",
      " cluster  num_shows                cluster_label                                                         top_keywords                     top_genres                                                         example_shows\n",
      "       0         40           family, love, life     family, love, life, downs, ups downs, ups, challenges, heartfelt         Comedy, Romance, Drama Glee, Californication, Last Man Standing, Nashville, Red Band Society\n",
      "       1         49     supernatural, dark, town    supernatural, dark, town, world, secrets, forces, navigates, life        Drama, Horror, Thriller                     Under the Dome, Bitten, Revenge, Grimm, Lost Girl\n",
      "       2         43         crime, justice, team           crime, justice, team, high, stakes, world, navigate, drama           Drama, Crime, Action       Person of Interest, True Detective, Homeland, Gotham, Continuum\n",
      "       3         31 diverse, humanity, survivors    diverse, humanity, survivors, new, group, face, challenges, earth Science-Fiction, Action, Drama           The 100, The Amazing Race, The Strain, The Last Ship, Helix\n",
      "       4         37  personal, world, challenges personal, world, challenges, family, navigates, life, explores, city           Drama, Crime, Comedy               Arrow, The Flash, Forever, Madam Secretary, The Lottery \n",
      "\n",
      "‚úÖ Sample of merged data with cluster labels:\n",
      "   id                name  cluster                 cluster_label\n",
      "0   1      Under the Dome      1.0      supernatural, dark, town\n",
      "1   2  Person of Interest      2.0          crime, justice, team\n",
      "2   3              Bitten      1.0      supernatural, dark, town\n",
      "3   4               Arrow      4.0   personal, world, challenges\n",
      "4   5      True Detective      2.0          crime, justice, team\n",
      "5   6             The 100      3.0  diverse, humanity, survivors\n",
      "6   7            Homeland      2.0          crime, justice, team\n",
      "7   8                Glee      0.0            family, love, life\n",
      "8   9             Revenge      1.0      supernatural, dark, town\n",
      "9  10               Grimm      1.0      supernatural, dark, town \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# SUMMARIZE EACH CLUSTER\n",
    "# ---------------------------------------------------------\n",
    "profiles = []\n",
    "for c, sub in df.groupby(\"cluster\", sort=True):\n",
    "    row_idx = sub.index.values\n",
    "    keywords = top_keywords_for_cluster(row_idx, topk=8)\n",
    "    top_genres = top_genres_for_cluster(sub, k=3)\n",
    "    examples = sub[\"name\"].head(5).tolist()\n",
    "    label = \", \".join(keywords[:3]) if keywords else f\"Cluster {c}\"\n",
    "\n",
    "    profiles.append({\n",
    "        \"cluster\": int(c),\n",
    "        \"num_shows\": int(len(sub)),\n",
    "        \"cluster_label\": label,\n",
    "        \"top_keywords\": \", \".join(keywords),\n",
    "        \"top_genres\": \", \".join(top_genres),\n",
    "        \"example_shows\": \", \".join(examples),\n",
    "    })\n",
    "\n",
    "profiles_df = pd.DataFrame(profiles).sort_values(\"cluster\").reset_index(drop=True)\n",
    "\n",
    "# ---- Show on screen\n",
    "print(\"üìã Cluster profiles preview:\")\n",
    "print(profiles_df.head(10).to_string(index=False), \"\\n\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ADD LABELS BACK TO FULL DATA\n",
    "# ---------------------------------------------------------\n",
    "labels_map = dict(zip(profiles_df[\"cluster\"], profiles_df[\"cluster_label\"]))\n",
    "df[\"cluster_label\"] = df[\"cluster\"].map(labels_map)\n",
    "\n",
    "print(\"‚úÖ Sample of merged data with cluster labels:\")\n",
    "print(df[[\"id\", \"name\", \"cluster\", \"cluster_label\"]].head(10), \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "269b59ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: C:\\Users\\brethm01\\tv-nlp\\src\\data\\shows_with_cluster_labels.parquet\n",
      "‚úÖ Saved: C:\\Users\\brethm01\\tv-nlp\\src\\data\\cluster_profiles.csv\n",
      "üìù Saved: C:\\Users\\brethm01\\tv-nlp\\src\\reports\\cluster_profiles.md\n"
     ]
    }
   ],
   "source": [
    "# ---- Save enriched per-show data (for later classification and analysis)\n",
    "out_parquet = DATA_DIR / \"shows_with_cluster_labels.parquet\"\n",
    "df.to_parquet(out_parquet, index=False)\n",
    "print(\"‚úÖ Saved:\", out_parquet.resolve())\n",
    "\n",
    "# ---- Save compact cluster profiles\n",
    "out_csv = DATA_DIR / \"cluster_profiles.csv\"\n",
    "profiles_df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "print(\"‚úÖ Saved:\", out_csv.resolve())\n",
    "\n",
    "# ---- Also save a slide-ready Markdown summary\n",
    "md_path = REPORTS / \"cluster_profiles.md\"\n",
    "with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Cluster Profiles\\n\\n\")\n",
    "    for _, r in profiles_df.iterrows():\n",
    "        f.write(f\"## Cluster {r['cluster']} ‚Äî {r['cluster_label']}  \\n\")\n",
    "        f.write(f\"- **#Shows:** {r['num_shows']}\\n\")\n",
    "        if r[\"top_genres\"]:\n",
    "            f.write(f\"- **Top Genres:** {r['top_genres']}\\n\")\n",
    "        f.write(f\"- **Top Keywords:** {r['top_keywords']}\\n\")\n",
    "        f.write(f\"- **Example Shows:** {r['example_shows']}\\n\\n\")\n",
    "print(\"üìù Saved:\", md_path.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64806730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loaded: 485 shows across 5 clusters\n",
      "\n",
      "üìã Top TF-IDF keywords per cluster:\n",
      "\n",
      " cluster                                                                           top_keywords\n",
      "       0      family, love, life, downs, ups downs, ups, challenges, heartfelt, quirky, moments\n",
      "       1      supernatural, dark, town, world, secrets, forces, navigates, life, past, identity\n",
      "       2           crime, justice, team, high, stakes, world, navigate, drama, new, high stakes\n",
      "       3 diverse, humanity, survivors, new, group, face, challenges, earth, thrilling, navigate\n",
      "       4     personal, world, challenges, family, navigates, life, explores, city, drama, sharp\n",
      "\n",
      "‚úÖ Saved: C:\\Users\\brethm01\\tv-nlp\\src\\data\\top_keywords_per_cluster.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pathlib import Path\n",
    "\n",
    "# Load clustered data (after merging and KMeans)\n",
    "df = pd.read_parquet(\"data/shows_with_umap_kmeans.parquet\")   # id, name, u1, u2, cluster\n",
    "review = pd.read_csv(\"data/shows_for_review.csv\")             # id, name, ai_summary\n",
    "merged = pd.merge(review, df[[\"id\", \"cluster\"]], on=\"id\", how=\"left\")\n",
    "\n",
    "# Clean missing summaries\n",
    "merged[\"ai_summary\"] = merged[\"ai_summary\"].fillna(\"\")\n",
    "\n",
    "print(f\"‚úÖ Data loaded: {len(merged)} shows across {merged['cluster'].nunique()} clusters\\n\")\n",
    "\n",
    "# --- Build TF-IDF model ---\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),     # unigrams + bigrams\n",
    "    stop_words=\"english\",\n",
    "    min_df=2,\n",
    "    max_df=0.8\n",
    ")\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(merged[\"ai_summary\"])\n",
    "terms = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "# --- Compute top keywords per cluster ---\n",
    "def top_keywords_for_cluster(cluster_id, topk=10):\n",
    "    rows = merged.index[merged[\"cluster\"] == cluster_id].tolist()\n",
    "    if not rows:\n",
    "        return []\n",
    "    mean_tfidf = X_tfidf[rows].mean(axis=0).A1\n",
    "    top_idx = np.argsort(-mean_tfidf)[:topk]\n",
    "    return terms[top_idx].tolist()\n",
    "\n",
    "results = []\n",
    "for c in sorted(merged[\"cluster\"].dropna().unique()):\n",
    "    keywords = top_keywords_for_cluster(int(c), topk=10)\n",
    "    results.append({\"cluster\": int(c), \"top_keywords\": \", \".join(keywords)})\n",
    "\n",
    "keywords_df = pd.DataFrame(results)\n",
    "\n",
    "# --- Show preview ---\n",
    "print(\"üìã Top TF-IDF keywords per cluster:\\n\")\n",
    "print(keywords_df.to_string(index=False))\n",
    "\n",
    "# --- Save for analysis / slides ---\n",
    "out_path = Path(\"data/top_keywords_per_cluster.csv\")\n",
    "keywords_df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\n‚úÖ Saved: {out_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c304051",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from wordcloud import WordCloud\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#for _, row in keywords_df.iterrows():\n",
    "#    plt.figure(figsize=(5, 5))\n",
    "#    wc = WordCloud(width=600, height=400, background_color=\"white\").generate(row[\"top_keywords\"])\n",
    "#    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "#    plt.axis(\"off\")\n",
    "#    plt.title(f\"Cluster {row['cluster']} ‚Äî Top Keywords\")\n",
    "#    plt.tight_layout()\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90de8593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
