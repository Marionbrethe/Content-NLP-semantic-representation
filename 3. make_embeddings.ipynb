{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99547209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1193385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentence-transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70188be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input dataset with summaries\n",
    "INP = Path(\"data/shows_merged.parquet\")\n",
    "\n",
    "# Output directory and file paths\n",
    "VEC = Path(\"vectors/summaries.npy\")                  # embeddings array\n",
    "INDEX_PATH = Path(\"vectors/summaries_index.parquet\") # show ID/name index\n",
    "VEC.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model choice\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"  # or another Sentence Transformer model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f3793da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current working directory: c:\\Users\\brethm01\\tv-nlp\\src\n",
      "Reading input file: C:\\Users\\brethm01\\tv-nlp\\src\\data\\shows_merged.parquet\n",
      "Rows with ai_summary: 200\n",
      "Saved index: C:\\Users\\brethm01\\tv-nlp\\src\\vectors\\summaries_index.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 1/1 [00:03<00:00,  3.45s/show]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings to: C:\\Users\\brethm01\\tv-nlp\\src\\vectors\\summaries.npy\n",
      "Shape: (200, 384)  (num_shows x dim=384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main(batch_size=256):\n",
    "    print(f\"\\nCurrent working directory: {Path.cwd()}\")\n",
    "    print(f\"Reading input file: {INP.resolve()}\")\n",
    "\n",
    "    # --- Load only needed columns ---\n",
    "    df = pd.read_parquet(INP, columns=[\"ai_summary\", \"id\", \"name\"])\n",
    "\n",
    "    # --- Drop empty summaries ---\n",
    "    df = df[df[\"ai_summary\"].notna() & (df[\"ai_summary\"].str.strip() != \"\")].reset_index(drop=True)\n",
    "    texts = df[\"ai_summary\"].tolist()\n",
    "\n",
    "    print(f\"Rows with ai_summary: {len(texts)}\")\n",
    "    if len(texts) == 0:\n",
    "        print(\"No summaries found to embed. Check that your merge and summarization steps ran correctly.\")\n",
    "        return\n",
    "\n",
    "    # --- Save an index file for later (so rows align with embeddings) ---\n",
    "    df[[\"id\", \"name\"]].to_parquet(INDEX_PATH, index=False)\n",
    "    print(f\"Saved index: {INDEX_PATH.resolve()}\")\n",
    "\n",
    "    # --- Load model and compute embeddings ---\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    dim = model.get_sentence_embedding_dimension()\n",
    "    embeddings = []\n",
    "\n",
    "    for start in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\", unit=\"show\"):\n",
    "        batch = texts[start : start + batch_size]\n",
    "        embs = model.encode(\n",
    "            batch,\n",
    "            batch_size=batch_size,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        embeddings.append(embs)\n",
    "\n",
    "    X = np.vstack(embeddings).astype(np.float32)\n",
    "\n",
    "    # --- Save embeddings ---\n",
    "    np.save(VEC, X)\n",
    "    print(f\"Saved embeddings to: {VEC.resolve()}\")\n",
    "    print(f\"Shape: {X.shape}  (num_shows x dim={dim})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7690808a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['summaries.npy', 'summaries_index.parquet']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"vectors\"))\n",
    "print(os.listdir(\"vectors\") if os.path.exists(\"vectors\") else \"No folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7736c213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(200, 384)\n"
     ]
    }
   ],
   "source": [
    "# Checks \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.load(\"vectors/summaries.npy\")\n",
    "print(type(X))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ed1db17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00150473  0.11482527 -0.00861445  0.03905157  0.05578645 -0.00723386\n",
      "  0.04783944 -0.03702857  0.00209945 -0.02428848]\n"
     ]
    }
   ],
   "source": [
    "print(X[0][:10])  # first 10 numbers of the first vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3527f622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 name                                         ai_summary\n",
      "0      Under the Dome  **Under the Dome** follows the residents of Ch...\n",
      "1  Person of Interest  In \"Person of Interest,\" a reclusive billionai...\n",
      "2              Bitten  In *Bitten*, follow Elena Michaels, the world’...\n",
      "[[-0.00150473  0.11482527 -0.00861445 ...  0.05430867 -0.0103502\n",
      "   0.03073471]\n",
      " [-0.05603698 -0.02702111 -0.1008964  ... -0.0392012  -0.04705472\n",
      "  -0.03598273]\n",
      " [-0.00344613  0.01225147 -0.0466712  ... -0.07568082  0.03041628\n",
      "  -0.10293239]]\n"
     ]
    }
   ],
   "source": [
    "# attach them back to the shows \n",
    "\n",
    "\n",
    "df = pd.read_parquet(\"data/shows_merged.parquet\")\n",
    "df = df[df[\"ai_summary\"].notna() & (df[\"ai_summary\"].str.strip() != \"\")].reset_index(drop=True)\n",
    "\n",
    "print(df[[\"name\", \"ai_summary\"]].head(3))\n",
    "print(X[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34efa68d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
